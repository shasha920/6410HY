{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EFWJFLDYcDCA"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"Thank you all so very much. Thank you to the Academy.\n",
        "               Thank you to all of you in this room. I have to congratulate\n",
        "               the other incredible nominees this year. The Revenant was\n",
        "               the product of the tireless efforts of an unbelievable cast\n",
        "               and crew. First off, to my brother in this endeavor, Mr. Tom\n",
        "               Hardy. Tom, your talent on screen can only be surpassed by\n",
        "               your friendship off screen … thank you for creating a t\n",
        "               ranscendent cinematic experience. Thank you to everybody at\n",
        "               Fox and New Regency … my entire team. I have to thank\n",
        "               everyone from the very onset of my career … To my parents;\n",
        "               none of this would be possible without you. And to my\n",
        "               friends, I love you dearly; you know who you are. And lastly,\n",
        "               I just want to say this: Making The Revenant was about\n",
        "               man's relationship to the natural world. A world that we\n",
        "               collectively felt in 2015 as the hottest year in recorded\n",
        "               history. Our production needed to move to the southern\n",
        "               tip of this planet just to be able to find snow. Climate\n",
        "               change is real, it is happening right now. It is the most\n",
        "               urgent threat facing our entire species, and we need to work\n",
        "               collectively together and stop procrastinating. We need to\n",
        "               support leaders around the world who do not speak for the\n",
        "               big polluters, but who speak for all of humanity, for the\n",
        "               indigenous people of the world, for the billions and\n",
        "               billions of underprivileged people out there who would be\n",
        "               most affected by this. For our children’s children, and\n",
        "               for those people out there whose voices have been drowned\n",
        "               out by the politics of greed. I thank you all for this\n",
        "               amazing award tonight. Let us not take this planet for\n",
        "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
      ],
      "metadata": {
        "id": "edbwxAB4cSZe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNcBI0PNcWP3",
        "outputId": "1d8dace5-0d6d-476a-efbb-8f675309887c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cYUjWEQdRWL",
        "outputId": "b464331b-0978-4859-9f9f-218ece366699"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ozNea5cdcaVl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first tokenize\n",
        "sentences=nltk.sent_tokenize(paragraph)#sentence tokenize"
      ],
      "metadata": {
        "id": "G93lqGURceGO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  words=nltk.word_tokenize(sentences[i])\n",
        "  newwords=[word for word in words if word not in stopwords.words('english')]\n",
        "  #for every word we add it to the newword list if those are not stop words\n",
        "  sentences[i]=' '.join(newwords)"
      ],
      "metadata": {
        "id": "MJLNsHM5dUPl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2NLNm_Nd6s4",
        "outputId": "85d1c9a9-c04a-4870-d718-7dc47141003d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Thank much .',\n",
              " 'Thank Academy .',\n",
              " 'Thank room .',\n",
              " 'I congratulate incredible nominees year .',\n",
              " 'The Revenant product tireless efforts unbelievable cast crew .',\n",
              " 'First , brother endeavor , Mr. Tom Hardy .',\n",
              " 'Tom , talent screen surpassed friendship screen … thank creating ranscendent cinematic experience .',\n",
              " 'Thank everybody Fox New Regency … entire team .',\n",
              " 'I thank everyone onset career … To parents ; none would possible without .',\n",
              " 'And friends , I love dearly ; know .',\n",
              " \"And lastly , I want say : Making The Revenant man 's relationship natural world .\",\n",
              " 'A world collectively felt 2015 hottest year recorded history .',\n",
              " 'Our production needed move southern tip planet able find snow .',\n",
              " 'Climate change real , happening right .',\n",
              " 'It urgent threat facing entire species , need work collectively together stop procrastinating .',\n",
              " 'We need support leaders around world speak big polluters , speak humanity , indigenous people world , billions billions underprivileged people would affected .',\n",
              " 'For children ’ children , people whose voices drowned politics greed .',\n",
              " 'I thank amazing award tonight .',\n",
              " 'Let us take planet granted .',\n",
              " 'I take tonight granted .',\n",
              " 'Thank much .']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSR8QQ1TeNet",
        "outputId": "3b86d795-cf69-49ea-a6a3-73d130c14656"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7IrWUAdeT41",
        "outputId": "36e2a3e0-7c6b-486b-bb57-16c71655cd5d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in practice sometime it is useful to create your own stop word list or edit the NLTK list\n",
        "stop_words=['in','of','at','a','the']\n",
        "sentences2=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "whm4nXO5eaBZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences2)):\n",
        "  words=nltk.word_tokenize(sentences2[i])\n",
        "  newwords=[word for word in words if word not in stop_words]\n",
        "  #for every word we add it to the newword list if those are not stop words\n",
        "  sentences2[i]=' '.join(newwords)"
      ],
      "metadata": {
        "id": "5l_w4MsCfHpF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences2[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8z2xtkPfAeC",
        "outputId": "925ff300-f99f-4d82-ce81-1cfe1d868df9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Thank you all so very much .',\n",
              " 'Thank you to Academy .',\n",
              " 'Thank you to all you this room .',\n",
              " 'I have to congratulate other incredible nominees this year .',\n",
              " 'The Revenant was product tireless efforts an unbelievable cast and crew .',\n",
              " 'First off , to my brother this endeavor , Mr. Tom Hardy .',\n",
              " 'Tom , your talent on screen can only be surpassed by your friendship off screen … thank you for creating t ranscendent cinematic experience .',\n",
              " 'Thank you to everybody Fox and New Regency … my entire team .',\n",
              " 'I have to thank everyone from very onset my career … To my parents ; none this would be possible without you .',\n",
              " 'And to my friends , I love you dearly ; you know who you are .',\n",
              " \"And lastly , I just want to say this : Making The Revenant was about man 's relationship to natural world .\",\n",
              " 'A world that we collectively felt 2015 as hottest year recorded history .',\n",
              " 'Our production needed to move to southern tip this planet just to be able to find snow .',\n",
              " 'Climate change is real , it is happening right now .',\n",
              " 'It is most urgent threat facing our entire species , and we need to work collectively together and stop procrastinating .',\n",
              " 'We need to support leaders around world who do not speak for big polluters , but who speak for all humanity , for indigenous people world , for billions and billions underprivileged people out there who would be most affected by this .',\n",
              " 'For our children ’ s children , and for those people out there whose voices have been drowned out by politics greed .',\n",
              " 'I thank you all for this amazing award tonight .',\n",
              " 'Let us not take this planet for granted .',\n",
              " 'I do not take tonight for granted .',\n",
              " 'Thank you so very much .']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}